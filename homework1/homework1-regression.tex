\documentclass[submit]{harvardml}

% Put in your full name and email address.
\name{Weiyi Chen}
\email{wec427@g.harvard.edu}

% List any people you worked with.
\collaborators{%
  N.A.
}

% You don't need to change these.
\course{CS181-S16}
\assignment{Assignment \#1}
\duedate{5:00pm February 5, 2016}

\usepackage[OT1]{fontenc}
\usepackage[colorlinks,citecolor=blue,urlcolor=blue]{hyperref}
\usepackage[pdftex]{graphicx}
\usepackage{subfig}
\usepackage{fullpage}
\usepackage{palatino}
\usepackage{mathpazo}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{color}
\usepackage{todonotes}
\usepackage{listings}
\usepackage{common}

\usepackage[mmddyyyy,hhmmss]{datetime}

\definecolor{verbgray}{gray}{0.9}

\lstnewenvironment{csv}{%
  \lstset{backgroundcolor=\color{verbgray},
  frame=single,
  framerule=0pt,
  basicstyle=\ttfamily,
  columns=fullflexible}}{}


\begin{document}
\begin{center}
{\Large Homework 1: Linear Regression}\\
\end{center}
You should submit your answers as a PDF via the Canvas course website.  There is a mathematical component and a programming component to this homework.You may collaborate with others, but are expected to list collaborators, and write up your problem sets individually.\\

\noindent
Please type your solutions after the corresponding problems using this \LaTeX\ template, and start each problem on a new page.\\

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Problem 1
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{problem}[Centering and Ridge Regression, 7pts]
%\subsection*{1. Centering and Ridge Regression [7pts]}

Consider a data set in which each data input vector $\boldx \in \mathbb{R}^m$. Let $\boldX \in
\mathbb{R}^{n \times m}$ be the input matrix, the rows of which are the input
vectors, and the columns of which are centered at 0. Let $\lambda$ be a positive constant. We define:

$$J(\boldw, w_0) = (\boldy - \boldX \boldw - w_0 {\bf 1 })^T (\boldy - \boldX
\boldw - w_0 {\bf 1 }) + \lambda \boldw^T \boldw$$

\begin{itemize}
  \item[(a)] Compute the gradient of $J(\boldw, w_0)$ with respect to $w_0$.
    Simplify as much as you can for full credit.
  \item[(b)] Compute the gradient of $J(\boldw, w_0)$ with respect to $\boldw$.
    Simplify as much as you can for full credit. Make sure to give your answer
    in matrix form.
  \item[(c)] Suppose that $\lambda > 0$. Knowing that $J$ is a convex function
    of its arguments, conclude that a global optimizer of
    $J(\boldw, w_0)$ is
    \begin{align}
      w_0 &= \frac{1}{n} \sum_i y_i \\
      \boldw &= (\boldX^T \boldX + \lambda \boldI)^{-1} \boldX^T \boldy
    \end{align}
    Before taking the inverse of a matrix, prove that it is invertible.
\end{itemize}
\end{problem}

\subsubsection*{Solution}

\begin{itemize}
  \item[(a)] The gradient of $J(w, w_0)$ with respect to $w_0$ is 
  \begin{align*}
    \nabla_{w_0} J(\boldw, w_0) &= 2(\boldy - \boldX\boldw - w_0\bold1)^T \cdot (-\bold1) \\
    &= -2(\boldy - \boldX\boldw - w_0\bold1)^T \bold1 \\
    &= -2(\boldy - \boldX\boldw)^T\bold1 + 2w_0n \\
    &= -2\boldy^T\cdot\bold1 + \boldw^T\boldX^T\bold1 + 2w_0n  \\
    &= -2\sum_iy_i + \boldw^T \bold0 + 2w_0n \\
    &= -2\sum_iy_i + 2w_0n
  \end{align*}
  
  \item[(b)] The gradient of $J(w, w_0)$ with respect to $\boldw$ is
  \begin{align*}
    \nabla_{\boldw} J(\boldw, w_0) &= 2(\boldy - \boldX\boldw - w_0\bold1)^T \cdot (-\boldX) + 2\lambda \boldw^T \\
    &= -2(\boldy - \boldX\boldw - w_0\bold1)^T \boldX + 2\lambda \boldw^T  \\
    &= 2\boldw^T(\boldX^T\boldX + \lambda\boldI) - 2\boldy^T\boldX + 2w_0\bold1^T\boldX \\
    &= 2\boldw^T(\boldX^T\boldX + \lambda\boldI) - 2\boldy^T\boldX + 2w_0\bold0 \\
    &= 2\boldw^T(\boldX^T\boldX + \lambda\boldI) - 2\boldy^T\boldX
  \end{align*}
  
  \item[(c)] A global optimizer of $J(\boldw, w_0)$ is derived by letting the above gradients as zero, i.e.
  \begin{align*}
    \nabla_{w_0} J(\boldw, w_0) = -2\sum_iy_i + 2w_0n = 0 &\Rightarrow w_0 = \frac{1}{n} \sum_i y_i \\
    \nabla_{\boldw} J(\boldw, w_0) = 2\boldw^T(\boldX^T\boldX + \lambda\boldI) - 2\boldy^T\boldX = \bold0 &\Rightarrow 
    \boldw = (\boldX^T \boldX + \lambda \boldI)^{-1} \boldX^T \boldy
  \end{align*}
  In order to prove the inverse of $\boldX^T \boldX + \lambda \boldI$ exists, we can verify its determinant is non-zero, i.e.
  $$ det(\boldX^T\boldX + \lambda\boldI) = |\boldX|^2 + \lambda > 0 $$ 
  given $|\boldX|^2 \ge 0$ and $\lambda > 0$, therefore $\boldX^T \boldX + \lambda \boldI$ is invertible.
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Problem 2
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
%\subsection*{2. Priors and Regularization [7pts]}
\begin{problem}[Priors and Regularization,7pts]
Consider the Bayesian linear regression model given in Bishop 3.3.1. The prior is
\begin{align*}
p(\boldw \given \alpha) = \mathcal{N}(\boldw \given \bold0, \alpha^{-1}\ident ),
\end{align*}
where $\alpha$ is the precision parameter that controls the variance of the Gaussian prior.  The likelihood can be written as
\begin{align*}
p(\boldt \given \boldw) &= \prod_{n=1}^N \mcN(t_n \given \boldw^\trans \bphi(\boldx_n), \beta^{-1}),
\end{align*}

Using the fact that the posterior is the product of the prior and the likelihood (up to a normalization constant), show that maximizing the log posterior (i.e., $\ln p(\boldw \given \boldt)= \ln p(\boldw | \alpha) + \ln p(\boldt \given \boldw)$) is equivalent to minimizing the regularized error term given by ${E_D(\boldw) + \lambda E_W(\boldw)}$ with 
\begin{align*}
E_D(\boldw) &= \frac{1}{2}\sum_{n = 1}^N (t_n - \boldw^\trans \bphi(\boldx_n))^2 \\
E_W(\boldw) &= \frac{1}{2}\boldw^\trans \boldw
\end{align*} 

Do this by writing $\ln p(\boldw \given \boldt)$ as a function of $E_D(\boldw)$ and $E_W(\boldw)$, dropping constant terms if necessary.  Conclude that maximizing this posterior is equivalent to minimizing the regularized error term given by $E_D(\boldw) + \lambda E_W(\boldw)$. (Hint: take $\lambda = \alpha/\beta$)
\end{problem}


\subsubsection*{Solution}

Write $\ln p(\boldw \given \boldt)$ as a function of $E_D(\boldw)$ and $E_W(\boldw)$, i.e.
\begin{align*}
  \ln p(\boldw \given \boldt) &= \ln p(\boldw \given \alpha) + \ln p(\boldt \given \boldw) \\
  &= \ln \mathcal{N}(\boldw \given \bold0, \alpha^{-1}\ident) + \ln \prod_{n=1}^N \mcN(t_n \given \boldw^\trans \bphi(\boldx_n), \beta^{-1}) \\
  &= \ln \mathcal{N}(\boldw \given \bold0, \alpha^{-1}\ident) + \sum_{n=1}^N \ln \mcN(t_n \given \boldw^\trans \bphi(\boldx_n), \beta^{-1})
\end{align*}
where the multivariate normal distribution formula is 
$$
\mathcal{N}(\boldx \given \boldsymbol\mu, \boldsymbol\Sigma) = \frac{1}{\sqrt{(2\pi)^{n}|\boldsymbol\Sigma|}}
\exp\left(-\frac{1}{2}({\mathbf x}-{\boldsymbol\mu})^\mathrm{T}{\boldsymbol\Sigma}^{-1}({\mathbf x}-{\boldsymbol\mu})
\right)
$$
via substituting, we derive
\begin{align*}
  \ln p(\boldw \given \boldt) = \left(\ln \frac{1}{\sqrt{(2\pi)^{n}\alpha^{-n}}} - \frac{1}{2}\alpha\boldw^T\boldw \right) + \left(\ln \frac{1}{\sqrt{(2\pi)^{n}\beta}} - \frac{1}{2}\beta\sum_{n = 1}^N (t_n - \boldw^\trans \bphi(\boldx_n))^2 \right)
\end{align*}

Dropping the constant terms, we have
\begin{align*}
  \ln p(\boldw \given \boldt) - \boldC &= - \frac{1}{2}\alpha\boldw^T\boldw - \frac{1}{2}\beta\sum_{n = 1}^N (t_n - \boldw^\trans \bphi(\boldx_n))^2 \\
  &= -\alpha E_W(\boldw) - \beta E_D(\boldw)
\end{align*}

Taking $\lambda = \alpha / \beta$, 
$$
\frac{1}{\beta}\left(\ln p(\boldw \given \boldt) - \boldC\right) = -\lambda E_W(\boldw) - E_D(\boldw)
$$

Therefore, maximizing the posterior is equivalent to maximize $-\lambda E_W(\boldw) - E_D(\boldw)$, or in other words, minimize the regularized error terms, i.e. $E_D(\boldw) + \lambda E_W(\boldw)$.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Problem 3
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\subsection*{3. Modeling Changes in Congress [10pts]}
 The objective of this problem is to learn about linear regression with basis
 functions by modeling the average age of the US Congress. The file
 \verb|congress-ages.csv| contains the data you will use for this problem.  It
 has two columns.  The first one is an integer that indicates the Congress
 number. Currently, the 114th Congress is in session. The second is the average
 age of that members of that Congress.  The data file looks like this:
\begin{csv}
congress,average_age
80,52.4959
81,52.6415
82,53.2328
83,53.1657
84,53.4142
85,54.1689
86,53.1581
87,53.5886
\end{csv}
and you can see a plot of the data in Figure~\ref{fig:congress}.

\begin{figure}[h]
\centering
\includegraphics[width=0.8\textwidth]{congress-ages}
\caption{Average age of Congress.  The horizontal axis is the Congress number, and the vertical axis is the average age of the congressmen.}
\label{fig:congress}
\end{figure}

\begin{problem}[Modeling Changes in Congress, 10pts]
Implement basis function regression with ordinary least squares with the above
data. Some sample Python code is provided in \verb|linreg.py|, which implements
linear regression.  Plot the data and regression lines for the simple linear
case, and for each of the following sets of basis functions:
\begin{itemize}
  \item[(a)] $\phi_j(x) = x^j$ for $j=1, \ldots, 7$
  \item[(b)] $\phi_j(x) = x^j$ for $j=1, \ldots, 3$
  \item[(c)] $\phi_j(x) = \sin\{ x / j \}$ for $j=1, \ldots, 4$
  \item[(d)] $\phi_j(x) = \sin\{ x / j \}$ for $j=1, \ldots, 7$
  \item[(e)] $\phi_j(x) = \sin\{ x / j \}$ for $j=1, \ldots, 20$
\end{itemize}
  In addition to the plots, provide one or two sentences for each, explaining
  whether you think it is fitting well, overfitting or underfitting.  If it does
  not fit well, provide a sentence explaining why. A good fit should capture the
  most important trends in the data.
  \end{problem}

\subsubsection*{Solution}
 Basis function regression is implemented in \verb|linreg.py|. Below are the plots for each case - 

\begin{figure}[h]
\centering
\includegraphics[width=0.45\textwidth]{congress-ages-polynomial-1}
\includegraphics[width=0.45\textwidth]{congress-ages-polynomial-7}
\caption{The data and regression line for the simple linear case and (a)}
\end{figure}

\begin{figure}[h]
\includegraphics[width=0.45\textwidth]{congress-ages-polynomial-3}
\includegraphics[width=0.45\textwidth]{congress-ages-sin-4}
\caption{The data and regression line for cases (b) and (c)}
\end{figure}

\begin{figure}[h]
\includegraphics[width=0.45\textwidth]{congress-ages-sin-7}
\includegraphics[width=0.45\textwidth]{congress-ages-sin-20}
\caption{The data and regression line for cases (d) and (e)}
\end{figure}

Following is my thinking of fitting well or not for each case - 
\begin{itemize}
  \item Fitting well: case (a), case (b), case (c), case (d)
  \item Overfitting: case (e)
  \item Underfitting: the simple linear case
\end{itemize}

The simple linear case does not fit well because it cannot capture the most important trends in the data, most of points are off from the line far away and the straight line cannot capture the curve indicated by the points either.

Case (e) is overfitting, though it captures every point exactly, it also fit to the error of each point, which is not expected. When the regression line goes out of the x range of the data points, the curve is going off far away to negative values very quickly, which is clearly unreasonable from our perspective.

\newpage
\begin{problem}[Calibration, 1pt]
Approximately how long did this homework take you to complete?
\end{problem}
\textbf{Answer:}
3 hours. Roughly 1 hour for each question, including investigation time on unfamiliar knowledge. 
\end{document}
