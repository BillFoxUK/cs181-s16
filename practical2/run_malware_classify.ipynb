{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# test sample code\n",
    "#%run -i sample-code.py\n",
    "malware_classes = [\"Agent\", \"AutoRun\", \"FraudLoad\", \"FraudPack\", \"Hupigon\", \"Krap\",\n",
    "           \"Lipler\", \"Magania\", \"None\", \"Poison\", \"Swizzor\", \"Tdss\",\n",
    "           \"VB\", \"Virut\", \"Zbot\"]\n",
    "val = \"\"\n",
    "for cls in malware_classes:\n",
    "    val += cls + \",\"\n",
    "\n",
    "feat_imp = np.loadtxt( \"feature_importances.csv\", delimiter = \",\"  )\n",
    "pct = np.percentile(feat_imp,40)\n",
    "imp_idx = feat_imp > 0\n",
    "len(feat_imp[imp_idx])\n",
    "\n",
    "len( t_train ) /2\n",
    "\n",
    "val\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sample_code as sc\n",
    "import numpy as np\n",
    "import cPickle\n",
    "reload( sc )\n",
    "#find valid tags, and save it, just run it once\n",
    "#sc.find_all_tags(0, 10000, direc=\"train\")\n",
    "#valid_tags = np.load( \"valid_tags.npy\" )\n",
    "\n",
    "#with open(\"valid_tags.pckl\") as f:\n",
    "#    valid_tags = cPickle.load(f)\n",
    "\n",
    "# extract features for both training and testing samples, just run once, unless to extract other features\n",
    "# sc.save_feature_data()\n",
    "\n",
    "# load data\n",
    "with open(\"train_features.pckl\") as f:\n",
    "    X_train, t_train, train_ids = cPickle.load(f)\n",
    "\n",
    "with open(\"test_features.pckl\") as f:\n",
    "    X_test, t_test, test_ids = cPickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# use sklearn, RF, different parameters, test in-sample and out-sample results, look at mis-classification and important features\n",
    "# and matrix of misclassification rates\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression, Ridge\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, GradientBoostingClassifier\n",
    "from LogisticRegression import LogisticRegression\n",
    "from GaussianGenerativeModel import GaussianGenerativeModel\n",
    "from sklearn.svm import SVC\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SV_poly_cont_tr_1hf Degree: 3 C: 0.0001 gamma: 0.5 coef0: 0.0 in-sp error: 0.0654569021387 out-sp error: 0.179520414776\n",
      "SV_poly_cont_tr_1hf Degree: 3 C: 0.0001 gamma: 1.0 coef0: 0.0 in-sp error: 0.0661049902787 out-sp error: 0.165262475697\n",
      "SV_poly_cont_tr_1hf Degree: 3 C: 0.0001 gamma: 2.0 coef0: 0.0 in-sp error: 0.143875567077 out-sp error: 0.230071289695\n",
      "SV_poly_cont_tr_1hf Degree: 3 C: 0.0001 gamma: 3.0 coef0: 0.0 in-sp error: 0.145819831497 out-sp error: 0.230719377835\n"
     ]
    }
   ],
   "source": [
    "# test different algo and get error stats\n",
    "def eval_results( bmk, pred ):\n",
    "    err = 0.0\n",
    "    mat = np.zeros((15,15))\n",
    "    for idx, val in enumerate(pred):\n",
    "        act = bmk[idx]\n",
    "        if act != val:\n",
    "            err += 1.0\n",
    "            mat[int(act),int(val)] += 1.0\n",
    "    err /= len(bmk)\n",
    "    return err, mat\n",
    "\n",
    "method = \"GG\"\n",
    "method = \"LR\"\n",
    "method = \"ET\"\n",
    "method = \"RF\"\n",
    "method = \"SV\"\n",
    "\n",
    "tot = len( t_train )\n",
    "tr_set = range(0,tot/2)\n",
    "val_set = range(tot/2,tot)\n",
    "#val_set,tr_set = tr_set,val_set\n",
    "prefix = method + \"_poly_cont_tr_1hf\"\n",
    "X_train_enh = X_train.copy()\n",
    "# change to binary\n",
    "#X_train_enh[ X_train_enh > 0.0 ] = 1.0\n",
    "\n",
    "# normalize\n",
    "#shp = X_train_enh.shape\n",
    "#X_train_enh = X_train_enh / X_train_enh.dot( np.ones( ( shp[1], shp[1] ) ) )\n",
    "\n",
    "# important features only\n",
    "feat_imp = np.loadtxt( \"feature_importances.csv\", delimiter = \",\"  )\n",
    "pct = np.percentile(feat_imp,40)\n",
    "imp_idx = feat_imp > pct\n",
    "# X_train_enh = X_train_enh[:,imp_idx]\n",
    "\n",
    "if( method == \"RF\" ):\n",
    "    n_est = 30\n",
    "    n_ft = 10 # 30 or 10 both seem OK     \n",
    "    min_split = 4 # min\n",
    "    if True: # for min_split in [1,2,3,4,5,6]:\n",
    "        regr = RandomForestClassifier(n_estimators = n_est, max_features = n_ft, min_samples_split = min_split )\n",
    "        regr.fit( X_train_enh[tr_set,:], t_train[tr_set] )\n",
    "        # get in sample and out of sample errors\n",
    "        Y_in = regr.predict( X_train_enh[tr_set,:] )\n",
    "        Y_out = regr.predict( X_train_enh[val_set,:] )\n",
    "        err_in, mat_in = eval_results( t_train[tr_set], Y_in )\n",
    "        err_out, mat_out = eval_results( t_train[val_set], Y_out )   \n",
    "        np.savetxt(prefix+\"_mat_in.csv\", mat_in, delimiter = ',')\n",
    "        np.savetxt(prefix+\"_mat_out.csv\", mat_out, delimiter = ',')\n",
    "        print prefix, \"n_est:\", n_est, \"max_ft\", n_ft, \"min_sp_spl\", min_split, \"in-sp error:\", err_in, \"out-sp error:\", err_out\n",
    "elif method == \"ET\":  \n",
    "    n_est = 30\n",
    "    n_ft = 30 # 30 or 10 both seem OK     \n",
    "    min_split = 4 # min\n",
    "    for n_est in [30]: #[10,20,30,40,50]:\n",
    "        for n_ft in [10]: # [5,10,20,30,40,50]:\n",
    "            for min_split in [4]: # [2,3,4,5,6,10,20]:\n",
    "                regr = ExtraTreesClassifier( n_estimators = n_est, max_features = n_ft, min_samples_split = min_split )\n",
    "                regr.fit( X_train_enh[tr_set,:], t_train[tr_set] )\n",
    "                # get in sample and out of sample errors\n",
    "                Y_in = regr.predict( X_train_enh[tr_set,:] )\n",
    "                Y_out = regr.predict( X_train_enh[val_set,:] )\n",
    "                err_in, mat_in = eval_results( t_train[tr_set], Y_in )\n",
    "                err_out, mat_out = eval_results( t_train[val_set], Y_out )  \n",
    "                print prefix, \"n_est:\", regr.n_estimators, \"max_ft\", regr.max_features, \"min_sp_spl\", regr.min_samples_split, \"in-sp error:\", err_in, \"out-sp error:\", err_out\n",
    "elif method == \"GG\":\n",
    "    flags = [True]\n",
    "    \n",
    "    for idx, shr_cov in enumerate(flags):\n",
    "        regr = GaussianGenerativeModel(isSharedCovariance=shr_cov)\n",
    "        regr.fit(X_train_enh[tr_set,:], t_train[tr_set])\n",
    "        # get in sample and out of sample errors\n",
    "        Y_in = regr.predict( X_train_enh[tr_set,:] )\n",
    "        Y_out = regr.predict( X_train_enh[val_set,:] )\n",
    "        err_in, mat_in = eval_results( t_train[tr_set], Y_in )\n",
    "        err_out, mat_out = eval_results( t_train[val_set], Y_out )  \n",
    "        np.savetxt(prefix+\"shrcov%d_mat_in.csv\" % idx, mat_in, delimiter = ',')\n",
    "        np.savetxt(prefix+\"shrcov%d_mat_out.csv\" % idx, mat_out, delimiter = ',')\n",
    "        print prefix, \"shared_covariance:\", shr_cov, \"in-sp error:\", err_in, \"out-sp error:\", err_out\n",
    "elif method == \"LR\":   \n",
    "    eta = 0.0001\n",
    "    for idx,lamda in enumerate([ 0, 0.0001, 0.001, 0.01, 0.1, 1 ]):\n",
    "        regr = LogisticRegression(eta=eta, lambda_parameter=lamda)\n",
    "        regr.fit(X_train_enh[tr_set,:], t_train[tr_set])\n",
    "        # get in sample and out of sample errors\n",
    "        Y_in = regr.predict( X_train_enh[tr_set,:] )\n",
    "        Y_out = regr.predict( X_train_enh[val_set,:] )\n",
    "        err_in, mat_in = eval_results( t_train[tr_set], Y_in )\n",
    "        err_out, mat_out = eval_results( t_train[val_set], Y_out )  \n",
    "        np.savetxt(prefix+\"%d_mat_in.csv\" % idx, mat_in, delimiter = ',')\n",
    "        np.savetxt(prefix+\"%d_mat_out.csv\" % idx, mat_out, delimiter = ',')\n",
    "        print prefix, \"lambda:\", lamda, \"in-sp error:\", err_in, \"out-sp error:\", err_out\n",
    "elif method == \"SV\":\n",
    "    rg = [0.01, 0.1, 1, 10, 100] +  [0.05, 0.5, 5, 50, 500] \n",
    "    rg = [1e-4,5e-4,1e-3,5e-3,1e-2]\n",
    "    rg = [1e-7, 1e-6,1e-5,1e-4]\n",
    "    rg = [5e-5,7.5e-5,1e-4,2e-4,3e-4,4e-4,5e-4,6e-4,7e-4,8e-4,9e-4]\n",
    "    rg = [9e-5,9.5e-5,10e-5]\n",
    "    rg = [1e-7,1e-6,1e-5]\n",
    "    rg = [0.01]\n",
    "    rg = [5.0]\n",
    "    rg = [0.01, 0.1, 1, 10, 100] +  [0.05, 0.5, 5, 50, 500] \n",
    "    rg = [ 1e-4 ]\n",
    "    rg.sort()\n",
    "    dgs = [3]\n",
    "    for dg in dgs:\n",
    "        for cval in rg: \n",
    "            for gm in [ 0.5, 1.0, 2.0, 3.0 ]:\n",
    "                for cf in [0.0 ]:\n",
    "                    for sp in [ None ]:\n",
    "                        regr = SVC(C=cval, kernel = 'poly',degree = dg, gamma = 1.0/102/gm, coef0 = cf )\n",
    "                        #regr = SVC(C=cval, kernel = 'sigmoid', gamma = gm, coef0 = cf )      \n",
    "                        #regr = SVC(C=cval, kernel = 'rbf', gamma = gm, coef0 = cf )                        \n",
    "                        regr.fit(X_train_enh[tr_set,:], t_train[tr_set])\n",
    "                        # get in sample and out of sample errors\n",
    "                        Y_in = regr.predict( X_train_enh[tr_set,:] )\n",
    "                        Y_out = regr.predict( X_train_enh[val_set,:] )\n",
    "                        err_in, mat_in = eval_results( t_train[tr_set], Y_in )\n",
    "                        err_out, mat_out = eval_results( t_train[val_set], Y_out )  \n",
    "                        #np.savetxt(prefix+\"_mat_in.csv\" , mat_in, delimiter = ',')\n",
    "                        #np.savetxt(prefix+\"_mat_out.csv\" , mat_out, delimiter = ',')\n",
    "                        print prefix, \"Degree:\", dg, \"C:\", cval, \"gamma:\", gm, \"coef0:\", cf, \"in-sp error:\", err_in, \"out-sp error:\", err_out\n",
    "\n",
    "#np.savetxt(prefix+\"_mat_in.csv\", mat_in, delimiter = ',')\n",
    "#np.savetxt(prefix+\"_mat_out.csv\", mat_out, delimiter = ',')\n",
    "#print prefix, \"n_est:\", n_est, \"max_ft\", n_ft, \"min_sp_spl\", min_split, \"in-sp error:\", err_in, \"out-sp error:\", err_out\n",
    "#print prefix, \"n_est:\", regr.n_estimators, \"max_ft\", regr.max_features, \"min_sp_spl\", regr.min_samples_split, \"in-sp error:\", err_in, \"out-sp error:\", err_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# saving feature importances from Random Forest\n",
    "#rfregr = regr\n",
    "#np.savetxt( \"feature_importances.csv\", rfregr.feature_importances_, delimiter = \",\" )\n",
    "\n",
    "#pct = np.percentile(rfregr.feature_importances_,rm_perc)\n",
    "#imp_idx = regr.feature_importances_ > pct\n",
    "#len(imp_idx[imp_idx])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In-sample error: 0.0184705119896\n"
     ]
    }
   ],
   "source": [
    "# Use selecte parameters to generate results\n",
    "from util import write_predictions\n",
    "method = \"RF\"\n",
    "method = \"SV\"\n",
    "method = \"ET\"\n",
    "\n",
    "if( method == \"RF\" ):\n",
    "    regr = RandomForestClassifier(n_estimators = 30, max_features = 30, min_samples_split = 4 )\n",
    "    regr.fit( X_train, t_train )\n",
    "    Y_test = regr.predict( X_test )\n",
    "    # write files for kaggles\n",
    "    write_predictions(Y_test, test_ids, \"rf_ft102_n30_mft30_spl4.csv\")  \n",
    "elif( method == \"ET\" ):\n",
    "    feat_imp = np.loadtxt( \"feature_importances.csv\", delimiter = \",\"  )\n",
    "    pct = np.percentile(feat_imp,40)\n",
    "    imp_idx = feat_imp > pct\n",
    "    regr = ExtraTreesClassifier(n_estimators = 30, max_features = 10, min_samples_split = 4 )  \n",
    " \n",
    "    # Use 60% of features\n",
    "    #regr.fit( X_train[:,imp_idx], t_train )\n",
    "    #Y_test = regr.predict( X_test[:,imp_idx] )\n",
    "    \n",
    "    #use full features\n",
    "    regr.fit( X_train, t_train )\n",
    "    Y_test = regr.predict( X_test )\n",
    "    \n",
    "    Y_in = regr.predict( X_train )\n",
    "    err_in, mat_in = eval_results( t_train, Y_in )\n",
    "    print \"In-sample error:\", err_in\n",
    "    \n",
    "    # write files for kaggles\n",
    "    write_predictions(Y_test, test_ids, \"et_ft102_n30_mft10_spl4.csv\")   \n",
    "elif( method == \"SV\"):\n",
    "    #regr = SVC(C=0.01, kernel = 'poly',degree = 2, gamma = 1.0/204, coef0 = 0.0 )\n",
    "    #regr = SVC(C=10.0, kernel = 'rbf', gamma = 1.0/(102*35), coef0 = 0.0 )\n",
    "    regr = SVC(C=1e-4, kernel = 'poly', degree = 3, gamma = 1.0/(102*1), coef0 = 0.0 )\n",
    "    regr.fit( X_train, t_train )\n",
    "    Y_test = regr.predict( X_test )\n",
    "    # write files for kaggles\n",
    "    write_predictions(Y_test, test_ids, \"svc_poly_C00001_deg3.csv\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# look at stats of t_train, to decide grouping of training set and validation set\n",
    "from collections import Counter\n",
    "all_perc=[]\n",
    "for i in [0,1]:\n",
    "    if i == 0:\n",
    "        t_set = t_train\n",
    "    else:    \n",
    "        t_set = t_train[0:len(t_train)/2]\n",
    "    ct = Counter(t_set)\n",
    "    tot = sum(ct.values(),0.0) / 100.0\n",
    "    for k in ct:\n",
    "        ct[k] /= tot \n",
    "    all_perc.append(ct)\n",
    "all_perc, all_perc[1] - all_perc[0], all_perc[0] -all_perc[1] # doesn't return negative value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    import xml.etree.cElementTree as ET\n",
    "except ImportError:\n",
    "    import xml.etree.ElementTree as ET\n",
    "\n",
    "direc = \"train\"\n",
    "index = 0\n",
    "i = -1\n",
    "X = None\n",
    "classes = []\n",
    "ids = [] \n",
    "for datafile in os.listdir(direc):\n",
    "    if datafile == '.DS_Store':\n",
    "        continue\n",
    "    i += 1\n",
    "    # extract id and true class (if available) from filename\n",
    "    id_str, clazz = datafile.split('.')[:2]\n",
    "    ids.append(id_str)\n",
    "    # add target class if this is training data\n",
    "    try:\n",
    "        classes.append(util.malware_classes.index(clazz))\n",
    "\n",
    "    except ValueError:\n",
    "        # we should only fail to find the label in our list of malware classes\n",
    "        # if this is test data, which always has an \"X\" label\n",
    "        assert clazz == \"X\"\n",
    "        classes.append(-1)\n",
    "    if i == index:\n",
    "        break\n",
    "\n",
    "tree = ET.parse(os.path.join(direc,datafile))\n",
    "tags = []\n",
    "for el in tree.iter():\n",
    "    call = el.tag\n",
    "    tags.append(call)\n",
    "tags = set(tags)\n",
    "valid_tags.symmetric_difference(tags)\n",
    "#el.getchildren()[0].getchildren()[0].getchildren()[0].getchildren()[0].items()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
